{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranieri-unimi/git.ammagamma/blob/main/WITHOUT_solution_TEXT_CLASSIFICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It0VEd1nm_ss"
      },
      "source": [
        "# Boring stuff: setting everything up\n",
        "\n",
        "*Warning: run this section only once*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLdduGRqOTkt"
      },
      "source": [
        "Connect to your Google Drive so that your work does not get lost when you end your session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyDyIrpcOxAp",
        "outputId": "f6276e17-e771-4feb-8575-0ca0b09b14b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDkZGzNFO86-"
      },
      "source": [
        "Change working directory to your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IQIWvXAnbHJ",
        "outputId": "bed2995b-af19-4499-a1fc-077d9272dc2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKo5JuZyPAgp"
      },
      "source": [
        "Create the main directory for the laboratory inside your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7D08GG8nOxN",
        "outputId": "778295c5-965a-46ef-90bd-c1392873f623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘NLP_MASTER’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir NLP_MASTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMqF_TKgPGZU"
      },
      "source": [
        "Remove unwanted directories (if it is your first run these directories do not exist and the following two commands have no effect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3sbhSc3RnWf"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/drive/MyDrive/NLP_MASTER/finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM0LeaHzWIIW"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/drive/MyDrive/NLP_MASTER/spacy-projects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcJb1vxPRgU"
      },
      "source": [
        "Now let's install all the dependencies for the laboratory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjdSfytkO30n",
        "outputId": "ee790f7e-cdd8-44e3-b15a-f3666b172174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy-nightly\n",
            "  Downloading spacy_nightly-3.0.0rc5-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (4.2.0)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 32.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (21.3)\n",
            "Collecting thinc<8.1.0,>=8.0.0\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (0.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (57.4.0)\n",
            "Collecting pathy\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.3.0\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (3.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0.dev0\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy-nightly) (1.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy-nightly) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy-nightly) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy-nightly) (2.10)\n",
            "Collecting typing-extensions>=3.7.4\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy-nightly) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy-nightly) (2.0.1)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-legacy, pathy, spacy-nightly\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires catalogue<1.1.0,>=0.0.7, but you have catalogue 2.0.7 which is incompatible.\n",
            "spacy 2.2.4 requires srsly<1.1.0,>=1.0.2, but you have srsly 2.4.3 which is incompatible.\n",
            "spacy 2.2.4 requires thinc==7.4.0, but you have thinc 8.0.17 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 pathy-0.6.1 pydantic-1.7.4 smart-open-5.2.1 spacy-legacy-3.0.9 spacy-nightly-3.0.0rc5 srsly-2.4.3 thinc-8.0.17 typer-0.3.2 typing-extensions-4.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy-nightly --pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "MyKK8ROgQwDJ",
        "outputId": "c48f74a5-178b-4530-89a9-e3a8805edc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.3.3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires catalogue<1.1.0,>=0.0.7, but you have catalogue 2.0.7 which is incompatible.\n",
            "spacy 2.2.4 requires srsly<1.1.0,>=1.0.2, but you have srsly 2.4.3 which is incompatible.\n",
            "spacy 2.2.4 requires thinc==7.4.0, but you have thinc 8.0.17 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.1.2 setuptools-62.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0O7OmXHRbaG",
        "outputId": "76666c57-4464-4606-c39d-17ef22adceb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.17)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (62.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.4)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: tokenizers, spacy-loggers, pyyaml, langcodes, huggingface-hub, transformers, spacy\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed huggingface-hub-0.7.0 langcodes-3.3.0 pyyaml-6.0 spacy-3.3.1 spacy-loggers-1.0.2 tokenizers-0.12.1 transformers-4.19.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mofd5GxBPXFa"
      },
      "source": [
        "Now that everything is set up, change working directory to the newly created directory NLP_MASTER in your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjw_FZuXDEOy",
        "outputId": "e3972ece-327e-496c-affe-2ac20e0364d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_MASTER\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP_MASTER/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPlHXzyLPgSv"
      },
      "source": [
        "Cline the official projects from the Spacy Repo, you are going to start from [this one](https://github.com/explosion/projects/tree/v3/tutorials/textcat_goemotions) and adapt it to the sentiment classification of financial news headlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTvvKjIbDfxm",
        "outputId": "7eb74dfb-6390-4104-f601-26045a3e90c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spacy-projects'...\n",
            "remote: Enumerating objects: 3742, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 3742 (delta 38), reused 73 (delta 32), pack-reused 3657\u001b[K\n",
            "Receiving objects: 100% (3742/3742), 13.48 MiB | 9.61 MiB/s, done.\n",
            "Resolving deltas: 100% (2293/2293), done.\n",
            "Checking out files: 100% (410/410), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/explosion/projects.git spacy-projects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJK3BhiXP8DT"
      },
      "source": [
        "Let's now create a subdirectory \"finance\" inside NLP_MASTER, where we are going to copy the textcat_goemotions tutorial we just cloned with git with the command above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOxbLb97D4x-"
      },
      "outputs": [],
      "source": [
        "!mkdir finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTp6GIMhDtKS"
      },
      "outputs": [],
      "source": [
        "!cp -r spacy-projects/tutorials/textcat_goemotions/* finance/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxhHBE5tEEoQ",
        "outputId": "8c2a4ad9-a134-46e4-a67c-e054ddb037ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_MASTER/finance\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP_MASTER/finance/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRzaWYD1QKfd"
      },
      "source": [
        "Spacy command line in action: now that we moved in the root directory of the project we tell Spacy to download everything the project needs in order to be run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9FRuUTRg1c",
        "outputId": "1b6681b7-9666-4459-afa1-fdc5f7c11c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Fetching 4 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset\n",
            "/content/drive/MyDrive/NLP_MASTER/finance/assets/categories.txt\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset\n",
            "/content/drive/MyDrive/NLP_MASTER/finance/assets/train.tsv\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset\n",
            "/content/drive/MyDrive/NLP_MASTER/finance/assets/dev.tsv\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset\n",
            "/content/drive/MyDrive/NLP_MASTER/finance/assets/test.tsv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!spacy project assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78DD2RmEcR2F"
      },
      "source": [
        "# Sentiment analysis: Reddit Posts Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPwXLgE9hv6G"
      },
      "source": [
        "*Example records [TEXT_CONTENT, EMOTION_ID, TEXT_ID]:*\n",
        "\n",
        "You can take a look at the dataset [here](https://drive.google.com/file/d/118kEBuOXikDJhlAvDVmAVxNBymtQ5MKb/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QqOVpUhtyb"
      },
      "source": [
        "*   My favourite food is anything I didn't have to cook myself.\t27\teebbqej \n",
        "*   Thank you friend\t15\teeqd04y\n",
        "*   It's crazy how far Photoshop has come. Underwater bridges?!! NEVER!!!\t7,13\tefanc6t\n",
        "\n",
        "\n",
        "Check out **assets/categories.txt** to explore the labels for this dataset. *The first row corresponds to the emotion_id 0, the second row to the emotion_id 1 and so on.*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJMuYICMaiqC"
      },
      "source": [
        "##***Edit [project.yml](/content/drive/MyDrive/NLP_MASTER/finance/project.yml) and change gpu_id from -1 to 0 in order to take advantage of the Colab GPU***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y96ESvsRtP2V"
      },
      "source": [
        "Let Spacy **preprocess Reddit Posts Dataset** (assets/train.tsv, assets/dev.tsv, assets/test.tsv and assets/categories.txt) and format it as it internally needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3yma_DPSBfT",
        "outputId": "b3a6fb91-3933-41dc-bc94-6998b1eb2594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "\u001b[38;5;4mℹ Skipping 'preprocess': nothing changed\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!spacy project run preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M7Z42aexBQP"
      },
      "source": [
        "Now that the dataset has been processed, **let's train the model** on the Reddit posts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs2nJiMGSSkH",
        "outputId": "a490d553-bf11-4bc5-aed5-3b5ba02c6bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train ./configs/cnn.cfg -o training/cnn --gpu-id 0\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/cnn\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-06-08 11:32:35,839] [INFO] Set up nlp object from config\n",
            "[2022-06-08 11:32:35,847] [INFO] Pipeline: ['textcat']\n",
            "[2022-06-08 11:32:35,851] [INFO] Created vocabulary\n",
            "[2022-06-08 11:32:35,852] [INFO] Finished initializing nlp object\n",
            "[2022-06-08 11:33:00,965] [INFO] Initialized pipeline components: ['textcat']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['textcat']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
            "---  ------  ------------  ----------  ------\n",
            "  0       0          0.30       50.18    0.50\n",
            "  0     200          8.00       55.12    0.55\n",
            "  0     400          7.09       58.45    0.58\n",
            "  0     600          6.66       61.43    0.61\n",
            "  0     800          6.48       62.51    0.63\n",
            "  0    1000          6.36       66.01    0.66\n",
            "  0    1200          6.21       68.30    0.68\n",
            "  1    1400          6.03       70.46    0.70\n",
            "  1    1600          5.65       72.40    0.72\n",
            "  1    1800          5.73       74.82    0.75\n",
            "  1    2000          5.81       76.27    0.76\n",
            "  1    2200          5.69       77.26    0.77\n",
            "  1    2400          5.62       78.00    0.78\n",
            "  1    2600          5.66       78.17    0.78\n",
            "  2    2800          5.44       78.32    0.78\n",
            "  2    3000          5.28       78.72    0.79\n",
            "  2    3200          5.33       79.17    0.79\n",
            "  2    3400          5.22       79.69    0.80\n",
            "  2    3600          5.36       79.74    0.80\n",
            "  2    3800          5.23       79.81    0.80\n",
            "  2    4000          5.32       80.04    0.80\n",
            "  3    4200          5.01       80.32    0.80\n",
            "  3    4400          4.93       80.57    0.81\n",
            "  3    4600          5.01       80.84    0.81\n",
            "  3    4800          4.96       81.10    0.81\n",
            "  3    5000          5.02       81.41    0.81\n",
            "  3    5200          5.06       81.69    0.82\n",
            "  3    5400          5.09       81.82    0.82\n",
            "  4    5600          4.67       82.00    0.82\n",
            "  4    5800          4.68       82.12    0.82\n",
            "  4    6000          4.74       82.14    0.82\n",
            "  4    6200          4.65       82.20    0.82\n",
            "  4    6400          4.84       82.34    0.82\n",
            "  4    6600          4.76       82.52    0.83\n",
            "  5    6800          4.70       82.69    0.83\n",
            "  5    7000          4.43       82.86    0.83\n",
            "  5    7200          4.42       82.97    0.83\n",
            "  5    7400          4.47       83.05    0.83\n",
            "  5    7600          4.52       83.14    0.83\n",
            "  5    7800          4.63       83.19    0.83\n",
            "  5    8000          4.48       83.22    0.83\n",
            "  6    8200          4.50       83.27    0.83\n",
            "  6    8400          4.13       83.36    0.83\n",
            "  6    8600          4.32       83.46    0.83\n",
            "  6    8800          4.27       83.58    0.84\n",
            "  6    9000          4.29       83.64    0.84\n",
            "  6    9200          4.40       83.69    0.84\n",
            "  6    9400          4.41       83.69    0.84\n",
            "  7    9600          4.09       83.64    0.84\n",
            "  7    9800          3.94       83.57    0.84\n",
            "  7   10000          4.13       83.54    0.84\n",
            "  7   10200          4.02       83.59    0.84\n",
            "  7   10400          4.13       83.58    0.84\n",
            "  7   10600          4.24       83.58    0.84\n",
            "  7   10800          4.27       83.60    0.84\n",
            "  8   11000          3.93       83.60    0.84\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/cnn/model-last\n"
          ]
        }
      ],
      "source": [
        "!spacy project run train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrWk8jo_xKt0"
      },
      "source": [
        "Automatic SpaCy evaluation of the model you just trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsUFT-TZWnUR",
        "outputId": "37a05649-3d4b-43af-cd0d-451b096ae310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate ./training/cnn/model-best ./corpus/test.spacy --output ./metrics/cnn.json\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK                   100.00\n",
            "TEXTCAT (macro AUC)   83.49 \n",
            "SPEED                 28793 \n",
            "\n",
            "\u001b[1m\n",
            "=========================== Textcat F (per label) ===========================\u001b[0m\n",
            "\n",
            "                      P       R       F\n",
            "admiration        64.56   60.71   62.58\n",
            "amusement         76.61   85.61   80.86\n",
            "anger             56.04   25.76   35.29\n",
            "annoyance         46.38   10.00   16.45\n",
            "approval          53.85   15.95   24.62\n",
            "caring            54.84   12.59   20.48\n",
            "confusion         44.90   14.38   21.78\n",
            "curiosity         52.38   30.99   38.94\n",
            "desire            51.35   22.89   31.67\n",
            "disappointment    66.67    1.32    2.60\n",
            "disapproval       40.00   14.98   21.80\n",
            "disgust           57.41   25.20   35.03\n",
            "embarrassment      0.00    0.00    0.00\n",
            "excitement        60.00   11.65   19.51\n",
            "fear              75.00   38.46   50.85\n",
            "gratitude         94.89   89.77   92.26\n",
            "grief              0.00    0.00    0.00\n",
            "joy               61.74   44.10   51.45\n",
            "love              77.08   81.93   79.43\n",
            "nervousness        0.00    0.00    0.00\n",
            "optimism          70.59   45.16   55.08\n",
            "pride              0.00    0.00    0.00\n",
            "realization      100.00    0.69    1.37\n",
            "relief             0.00    0.00    0.00\n",
            "remorse           58.18   57.14   57.66\n",
            "sadness           65.88   35.90   46.47\n",
            "surprise          57.83   34.04   42.86\n",
            "neutral           62.25   59.71   60.95\n",
            "\n",
            "\u001b[1m\n",
            "======================== Textcat ROC AUC (per label) ========================\u001b[0m\n",
            "\n",
            "                 ROC AUC\n",
            "admiration          0.88\n",
            "amusement           0.96\n",
            "anger               0.85\n",
            "annoyance           0.78\n",
            "approval            0.75\n",
            "caring              0.81\n",
            "confusion           0.87\n",
            "curiosity           0.90\n",
            "desire              0.87\n",
            "disappointment      0.78\n",
            "disapproval         0.83\n",
            "disgust             0.88\n",
            "embarrassment       0.76\n",
            "excitement          0.82\n",
            "fear                0.86\n",
            "gratitude           0.97\n",
            "grief               0.56\n",
            "joy                 0.87\n",
            "love                0.97\n",
            "nervousness         0.77\n",
            "optimism            0.88\n",
            "pride               0.72\n",
            "realization         0.78\n",
            "relief              0.77\n",
            "remorse             0.97\n",
            "sadness             0.87\n",
            "surprise            0.83\n",
            "neutral             0.81\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to metrics/cnn.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!spacy project run evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvPLr4kjmevs"
      },
      "source": [
        "Let's test the model on some examples, **feel free to change them to whatever you want**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP9ZWIoJXGmy",
        "outputId": "748a4dad-0c0b-419e-df3c-cc8dd1b65296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'admiration': 0.001072859624400735, 'amusement': 0.005732352379709482, 'anger': 0.007456898223608732, 'annoyance': 0.00208781729452312, 'approval': 0.0018803244456648827, 'caring': 0.04452180489897728, 'confusion': 0.0008455381612293422, 'curiosity': 0.003570307046175003, 'desire': 0.0202154740691185, 'disappointment': 0.0325545109808445, 'disapproval': 0.004504743497818708, 'disgust': 0.05633332580327988, 'embarrassment': 0.006672602612525225, 'excitement': 0.0031072725541889668, 'fear': 0.0030973341781646013, 'gratitude': 0.0028433476109057665, 'grief': 0.006908386945724487, 'joy': 0.003084561787545681, 'love': 0.002204897813498974, 'nervousness': 0.005881026852875948, 'optimism': 0.9313751459121704, 'pride': 0.0013122900854796171, 'realization': 0.0041063628159463406, 'relief': 0.005566315725445747, 'remorse': 0.015642797574400902, 'sadness': 0.1405908167362213, 'surprise': 0.0007496478501707315, 'neutral': 0.0026976754888892174}\n",
            "{'admiration': 0.07745881378650665, 'amusement': 0.010573511943221092, 'anger': 0.007202767301350832, 'annoyance': 0.019489947706460953, 'approval': 0.6058114171028137, 'caring': 0.03418494760990143, 'confusion': 0.01018020510673523, 'curiosity': 0.005243584979325533, 'desire': 0.008869204670190811, 'disappointment': 0.0104603860527277, 'disapproval': 0.006149083375930786, 'disgust': 0.009458623826503754, 'embarrassment': 0.001926710014231503, 'excitement': 0.013814451172947884, 'fear': 0.0057523539289832115, 'gratitude': 0.024424606934189796, 'grief': 0.000727497274056077, 'joy': 0.022904373705387115, 'love': 0.006906447000801563, 'nervousness': 0.0018886997131630778, 'optimism': 0.08149712532758713, 'pride': 0.008309885859489441, 'realization': 0.02577180042862892, 'relief': 0.0022988521959632635, 'remorse': 0.012145224027335644, 'sadness': 0.008868387900292873, 'surprise': 0.003549072425812483, 'neutral': 0.49770355224609375}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"./training/cnn/model-best\")\n",
        "\n",
        "texts = [\n",
        "    \"It was really bad to watch you leave, hopefully you'll be back soon\",\n",
        "    \"Oh yes, I can relate to that. Still, you'd better think about it twice.\",\n",
        "]\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    # Do something with the doc here\n",
        "    print(doc.cats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icfgI4EuxGNO"
      },
      "source": [
        "#Data Preparation: from the Reddit Post Dataset to the Financial News Dataset\n",
        "**TODO: Upload Financial News Dataset file FinancialPhraseBank_AllAgree.txt to the assets folder, you can find the dataset [here](https://drive.google.com/file/d/1WXM2t8sh-myIEUZt37zIXC2McNrCyS2l/view?usp=sharing)**\\\n",
        "\n",
        "Financial news dataset example records [TEXT_CONTENT, SENTIMENT_LABEL]:\n",
        "\n",
        "\n",
        "*   According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\n",
        "*   Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .@positive\n",
        "*   Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .@negative\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now you have to **format the Financial News Dataset like the Reddit Posts Dataset**, in order to retrain the sentiment classifier on the new financial dataset.\n",
        "\n",
        "Remember to split the dataset into train (70%), validation (10%) and test (20%), **saving the respective TSV files (train.tsv, dev.tsv, test.tsv) in the asset folder** .\n",
        "\n",
        "---\n",
        "## Hints:\n",
        "- Our final dataset should have the following columns: text, label, id. Text and label are already in our file (in the same row!), while the ID should be generated uniquely (e.g. use uuid.uuid4())\n",
        "- Categories are represented as strings (neutral, positive, negative), while spacy expects them as integer.\n",
        "- Should we split the observations randomly or use some specific criteria?\n",
        "- The train, val and test files should be stored as tab separated files (sep=\"\\t\") under the assets/ folder, with the following names: \n",
        "  - train.tsv\n",
        "  - dev.tsv\n",
        "  - test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOpT4WY7rIT6"
      },
      "outputs": [],
      "source": [
        "### TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQzb59pGk2F3"
      },
      "source": [
        "If you didn't do it before, check out the file under assets/categories.txt : it contains the (many) labels for the sentiment classification of the Reddit Posts Dataset, now you have to **change it to the labels of the Financial News Dataset (neutral, positive, negative)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf2gsGF5Pm5A"
      },
      "outputs": [],
      "source": [
        "#!echo -en \"neutral\\npositive\\nnegative\" > /content/drive/MyDrive/NLP_MASTER/finance/assets/categories.txt\n",
        "!echo -en \"neutral\\npositive\\nnegative\" > ./assets/categories.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOiyarkvlUsI"
      },
      "source": [
        "Let again Spacy **preprocess our input files** (assets/train.tsv, assets/dev.tsv, assets/test.tsv and assets/categories.txt) and format them as it internally needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfaExG4FNcMs"
      },
      "outputs": [],
      "source": [
        "!spacy project run preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__aHmybpl0_M"
      },
      "source": [
        "Spacy is a bit picky about existing directories, **delete the previous CNN model** you trained on the Reddit Posts Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGCpwpsGOUYE"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/drive/MyDrive/NLP_MASTER/finance/training/cnn\n",
        "!rm -rf ./training/cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeT1LSy7mCyF"
      },
      "source": [
        "Everything is ready, **let's train the model** on the Financial News Dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox9yjwooNcvT"
      },
      "outputs": [],
      "source": [
        "!spacy project run train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwYEsf64NeOm"
      },
      "outputs": [],
      "source": [
        "!spacy project run evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsukcqFNQMGt"
      },
      "source": [
        "# Running predictions on examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRpR7lhlNlKm"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"./training/cnn/model-best\")\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    # Do something with the doc here\n",
        "    print(doc.cats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KNgnXQoq4An"
      },
      "source": [
        "\n",
        "# Final task: sentiment as a Prophet regressor\n",
        "\n",
        "## Main goal\n",
        "**The presentations will start at 11:45 a.m. on Friday**\n",
        "Forecast the EUR-USD exchange rate, using both timeseries (e.g. previous values) and news Downloaded from [ForexRate news archive.](http://www.forexrate.co.uk/newsarchive.php). \n",
        "## Dataset\n",
        "You will use the dataset downloaded and used in previous labs.\n",
        "As a test set use observations in the range [1st June 2021, 1st June 2022], extremes included.\n",
        "\n",
        "In your presentation you should focus on the methodological approach you used for solving this problem **AND** the main insights to share with your business stakeholders.\n",
        "\n",
        "## Metrics\n",
        "You should use some of the metrics shown during the time-series lecture (or even better ones!) and motivate your choices. It will be certainly interesting to go beyond stating \"the MAE is X.Y\": are there any particular patterns? how performances varies throughout time? is it worth having a predictive model instead of \"baseline\" approaches?\n",
        "## Presentation format\n",
        "Each team, made of 3/4 members, will present their results to all of us in 15 minutes, using a brief Power point presentation and answer eventual questions (both from us and other teams!).\n",
        "\n",
        "## Organizational stuff\n",
        "**The presentations will start at 11:45 a.m. on Friday**\n",
        "\n",
        "Until then you can work together with your team mates: please don't work on it overnight!!!\n",
        "For us it's more interesting to see which insights will you share with business stakeholders and the statistical robustness of your methodological approaches, instead of seeing an infinitesimal improvement on your metrics of choice.\n",
        "\n",
        "We will be available also tomorrow morning, from 9 a.m., for answering all your questions and/or help you solve some technical issues on the dedicated call.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJNChcLoNnMo"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "offset = 0\n",
        "max_offset = 1649\n",
        "offset_increment = 12\n",
        "\n",
        "BASE_URL = 'http://www.forexrate.co.uk/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2_Ur1OwPVAw"
      },
      "outputs": [],
      "source": [
        "news_archive = []\n",
        "\n",
        "for i in range(0,max_offset,offset_increment):\n",
        "  url = f'http://www.forexrate.co.uk/newsarchive.php?start={i}'\n",
        "  print(url)\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  tables = soup.findChildren('table')\n",
        "  news_table = tables[1]\n",
        "  rows = news_table.findChildren(['th', 'tr'])\n",
        "\n",
        "  for idx,row in enumerate(rows):\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    cells = row.findChildren('td')\n",
        "    for idx,cell in enumerate(cells):\n",
        "      txt = cell.text\n",
        "      href = cell.find('a')['href']\n",
        "      href = BASE_URL + href.replace('./','')\n",
        "      if \"newsarchive.php?start=\" in href:\n",
        "        continue\n",
        "      # let's get the date of the article\n",
        "      date_page = requests.get(href)\n",
        "      date_soup = BeautifulSoup(date_page.content, 'html.parser')\n",
        "      date_div = date_soup.findChildren('div')[3]\n",
        "      date_str = date_div.text\n",
        "      news_archive.append({'txt':txt,'url':href,'date':date_str})\n",
        "      print(len(news_archive), date_str, {'txt':txt,'url':href,'date':date_str})\n",
        "      #print(value, href)\n",
        "  #print(len(news_archive))\n",
        "\n",
        "#print(news_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nECtBqqLPWDd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(news_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIoxepBrQ1xi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIBqDfsqhST0"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"./hist_fx.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Hd6mZXTOjL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_MASTER/news_archive.pkl', 'wb') as f:\n",
        "  pickle.dump(news_archive, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_MASTER/news_archive.pkl', 'rb') as f:\n",
        "  loaded_news_archive = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQgS-LtLpPy6"
      },
      "outputs": [],
      "source": [
        "len(loaded_news_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oNxEQjwpQwl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "WITHOUT_solution_TEXT_CLASSIFICATION.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}