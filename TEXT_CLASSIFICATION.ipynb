{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranieri-unimi/git.ammagamma/blob/main/TEXT_CLASSIFICATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It0VEd1nm_ss"
      },
      "source": [
        "# Boring stuff: setting everything up\n",
        "\n",
        "*Warning: run this section only once*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjdSfytkO30n"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-nightly --pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyKK8ROgQwDJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0O7OmXHRbaG"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTvvKjIbDfxm"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/explosion/projects.git spacy-projects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f9FRuUTRg1c"
      },
      "outputs": [],
      "source": [
        "!spacy project assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78DD2RmEcR2F"
      },
      "source": [
        "# Sentiment analysis: Reddit Posts Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPwXLgE9hv6G"
      },
      "source": [
        "*Example records [TEXT_CONTENT, EMOTION_ID, TEXT_ID]:*\n",
        "\n",
        "You can take a look at the dataset [here](https://drive.google.com/file/d/118kEBuOXikDJhlAvDVmAVxNBymtQ5MKb/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QqOVpUhtyb"
      },
      "source": [
        "*   My favourite food is anything I didn't have to cook myself.\t27\teebbqej \n",
        "*   Thank you friend\t15\teeqd04y\n",
        "*   It's crazy how far Photoshop has come. Underwater bridges?!! NEVER!!!\t7,13\tefanc6t\n",
        "\n",
        "\n",
        "Check out **assets/categories.txt** to explore the labels for this dataset. *The first row corresponds to the emotion_id 0, the second row to the emotion_id 1 and so on.*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJMuYICMaiqC"
      },
      "source": [
        "##***Edit [project.yml](/content/drive/MyDrive/NLP_MASTER/finance/project.yml) and change gpu_id from -1 to 0 in order to take advantage of the Colab GPU***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3yma_DPSBfT"
      },
      "outputs": [],
      "source": [
        "!spacy project run preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs2nJiMGSSkH"
      },
      "outputs": [],
      "source": [
        "!spacy project run train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsUFT-TZWnUR"
      },
      "outputs": [],
      "source": [
        "!spacy project run evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP9ZWIoJXGmy"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"./training/cnn/model-best\")\n",
        "\n",
        "texts = [\n",
        "    \"It was really bad to watch you leave, hopefully you'll be back soon\",\n",
        "    \"Oh yes, I can relate to that. Still, you'd better think about it twice.\",\n",
        "]\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    # Do something with the doc here\n",
        "    print(doc.cats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icfgI4EuxGNO"
      },
      "source": [
        "#Data Preparation: from the Reddit Post Dataset to the Financial News Dataset\n",
        "**TODO: Upload Financial News Dataset file FinancialPhraseBank_AllAgree.txt to the assets folder, you can find the dataset [here](https://drive.google.com/file/d/1WXM2t8sh-myIEUZt37zIXC2McNrCyS2l/view?usp=sharing)**\\\n",
        "\n",
        "Financial news dataset example records [TEXT_CONTENT, SENTIMENT_LABEL]:\n",
        "\n",
        "\n",
        "*   According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\n",
        "*   Finnish Talentum reports its operating profit increased to EUR 20.5 mn in 2005 from EUR 9.3 mn in 2004 , and net sales totaled EUR 103.3 mn , up from EUR 96.4 mn .@positive\n",
        "*   Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .@negative\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now you have to **format the Financial News Dataset like the Reddit Posts Dataset**, in order to retrain the sentiment classifier on the new financial dataset.\n",
        "\n",
        "Remember to split the dataset into train (70%), validation (10%) and test (20%), **saving the respective TSV files (train.tsv, dev.tsv, test.tsv) in the asset folder** .\n",
        "\n",
        "---\n",
        "## Hints:\n",
        "- Our final dataset should have the following columns: text, label, id. Text and label are already in our file (in the same row!), while the ID should be generated uniquely (e.g. use uuid.uuid4())\n",
        "- Categories are represented as strings (neutral, positive, negative), while spacy expects them as integer.\n",
        "- Should we split the observations randomly or use some specific criteria?\n",
        "- The train, val and test files should be stored as tab separated files (sep=\"\\t\") under the assets/ folder, with the following names: \n",
        "  - train.tsv\n",
        "  - dev.tsv\n",
        "  - test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOpT4WY7rIT6"
      },
      "outputs": [],
      "source": [
        "### TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQzb59pGk2F3"
      },
      "source": [
        "If you didn't do it before, check out the file under assets/categories.txt : it contains the (many) labels for the sentiment classification of the Reddit Posts Dataset, now you have to **change it to the labels of the Financial News Dataset (neutral, positive, negative)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf2gsGF5Pm5A"
      },
      "outputs": [],
      "source": [
        "#!echo -en \"neutral\\npositive\\nnegative\" > /content/drive/MyDrive/NLP_MASTER/finance/assets/categories.txt\n",
        "!echo -en \"neutral\\npositive\\nnegative\" > ./assets/categories.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOiyarkvlUsI"
      },
      "source": [
        "Let again Spacy **preprocess our input files** (assets/train.tsv, assets/dev.tsv, assets/test.tsv and assets/categories.txt) and format them as it internally needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfaExG4FNcMs"
      },
      "outputs": [],
      "source": [
        "!spacy project run preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__aHmybpl0_M"
      },
      "source": [
        "Spacy is a bit picky about existing directories, **delete the previous CNN model** you trained on the Reddit Posts Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGCpwpsGOUYE"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/drive/MyDrive/NLP_MASTER/finance/training/cnn\n",
        "!rm -rf ./training/cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeT1LSy7mCyF"
      },
      "source": [
        "Everything is ready, **let's train the model** on the Financial News Dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox9yjwooNcvT"
      },
      "outputs": [],
      "source": [
        "!spacy project run train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwYEsf64NeOm"
      },
      "outputs": [],
      "source": [
        "!spacy project run evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsukcqFNQMGt"
      },
      "source": [
        "# Running predictions on examples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRpR7lhlNlKm"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"./training/cnn/model-best\")\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "for doc in nlp.pipe(texts):\n",
        "    # Do something with the doc here\n",
        "    print(doc.cats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KNgnXQoq4An"
      },
      "source": [
        "\n",
        "# Final task: sentiment as a Prophet regressor\n",
        "\n",
        "## Main goal\n",
        "**The presentations will start at 11:45 a.m. on Friday**\n",
        "Forecast the EUR-USD exchange rate, using both timeseries (e.g. previous values) and news Downloaded from [ForexRate news archive.](http://www.forexrate.co.uk/newsarchive.php). \n",
        "## Dataset\n",
        "You will use the dataset downloaded and used in previous labs.\n",
        "As a test set use observations in the range [1st June 2021, 1st June 2022], extremes included.\n",
        "\n",
        "In your presentation you should focus on the methodological approach you used for solving this problem **AND** the main insights to share with your business stakeholders.\n",
        "\n",
        "## Metrics\n",
        "You should use some of the metrics shown during the time-series lecture (or even better ones!) and motivate your choices. It will be certainly interesting to go beyond stating \"the MAE is X.Y\": are there any particular patterns? how performances varies throughout time? is it worth having a predictive model instead of \"baseline\" approaches?\n",
        "## Presentation format\n",
        "Each team, made of 3/4 members, will present their results to all of us in 15 minutes, using a brief Power point presentation and answer eventual questions (both from us and other teams!).\n",
        "\n",
        "## Organizational stuff\n",
        "**The presentations will start at 11:45 a.m. on Friday**\n",
        "\n",
        "Until then you can work together with your team mates: please don't work on it overnight!!!\n",
        "For us it's more interesting to see which insights will you share with business stakeholders and the statistical robustness of your methodological approaches, instead of seeing an infinitesimal improvement on your metrics of choice.\n",
        "\n",
        "We will be available also tomorrow morning, from 9 a.m., for answering all your questions and/or help you solve some technical issues on the dedicated call.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJNChcLoNnMo"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "offset = 0\n",
        "max_offset = 1649\n",
        "offset_increment = 12\n",
        "\n",
        "BASE_URL = 'http://www.forexrate.co.uk/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2_Ur1OwPVAw"
      },
      "outputs": [],
      "source": [
        "news_archive = []\n",
        "\n",
        "for i in range(0,max_offset,offset_increment):\n",
        "  url = f'http://www.forexrate.co.uk/newsarchive.php?start={i}'\n",
        "  print(url)\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  tables = soup.findChildren('table')\n",
        "  news_table = tables[1]\n",
        "  rows = news_table.findChildren(['th', 'tr'])\n",
        "\n",
        "  for idx,row in enumerate(rows):\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    cells = row.findChildren('td')\n",
        "    for idx,cell in enumerate(cells):\n",
        "      txt = cell.text\n",
        "      href = cell.find('a')['href']\n",
        "      href = BASE_URL + href.replace('./','')\n",
        "      if \"newsarchive.php?start=\" in href:\n",
        "        continue\n",
        "      # let's get the date of the article\n",
        "      date_page = requests.get(href)\n",
        "      date_soup = BeautifulSoup(date_page.content, 'html.parser')\n",
        "      date_div = date_soup.findChildren('div')[3]\n",
        "      date_str = date_div.text\n",
        "      news_archive.append({'txt':txt,'url':href,'date':date_str})\n",
        "      print(len(news_archive), date_str, {'txt':txt,'url':href,'date':date_str})\n",
        "      #print(value, href)\n",
        "  #print(len(news_archive))\n",
        "\n",
        "#print(news_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nECtBqqLPWDd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(news_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIoxepBrQ1xi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIBqDfsqhST0"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"./hist_fx.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Hd6mZXTOjL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_MASTER/news_archive.pkl', 'wb') as f:\n",
        "  pickle.dump(news_archive, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP_MASTER/news_archive.pkl', 'rb') as f:\n",
        "  loaded_news_archive = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQgS-LtLpPy6"
      },
      "outputs": [],
      "source": [
        "len(loaded_news_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oNxEQjwpQwl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "WITHOUT_solution_TEXT_CLASSIFICATION.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}